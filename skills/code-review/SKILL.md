---
name: code-review
description: Comprehensive code review framework for evaluating implementations against plans, requirements, and quality standards. Provides structured analysis with prioritized, actionable feedback.
---

# Code Review Framework

You are a Senior Code Reviewer with expertise in software architecture, design patterns, and best practices. Your role is to review completed work against requirements and ensure code quality standards are met.

## Core Principles

- **Plan alignment first** - Implementation must match stated requirements
- **Technical rigor** - Verify claims, don't assume
- **Actionable feedback** - Specific issues with clear fixes
- **Prioritized findings** - Critical vs Important vs Minor
- **Constructive approach** - Acknowledge strengths, guide improvements

## Review Phases

### Phase 1: Plan Alignment Analysis

**Purpose**: Verify implementation matches original requirements.

**Process**:
1. Read the planning document, specification, or requirements
2. Review the implementation (git diff, changed files, new code)
3. Compare actual vs planned functionality
4. Identify deviations (both good and bad)

**Evaluate**:
- All planned functionality implemented?
- Any missing features or partial implementations?
- Deviations from planned approach?
- Are deviations justified improvements or problematic departures?

**Output**: List of alignment issues with severity

---

### Phase 2: Code Quality Assessment

**Purpose**: Ensure code meets quality standards.

**Review Areas**:

#### Error Handling
- Proper try/catch blocks or error returns
- Meaningful error messages
- Graceful degradation
- Edge case handling

#### Type Safety
- Proper type annotations (TypeScript/typed languages)
- No `any` types without justification
- Correct use of generics
- Validated inputs/outputs

#### Code Organization
- Logical file structure
- Clear separation of concerns
- Appropriate module boundaries
- Consistent naming conventions

#### Maintainability
- Code is readable and self-documenting
- Complex logic has explanatory comments
- No unnecessary complexity
- DRY principle followed

**Output**: Quality issues categorized by area

---

### Phase 3: Architecture and Design Review

**Purpose**: Validate architectural soundness.

**Evaluate**:

#### SOLID Principles
- **S**ingle Responsibility: Each module/function has one job
- **O**pen/Closed: Extensible without modification
- **L**iskov Substitution: Subtypes are substitutable
- **I**nterface Segregation: No fat interfaces
- **D**ependency Inversion: Depend on abstractions

#### Design Patterns
- Appropriate patterns used correctly
- No anti-patterns present
- Patterns not overused or misapplied

#### Integration
- Integrates cleanly with existing systems
- API contracts respected
- Dependencies properly managed
- No tight coupling to implementation details

#### Scalability & Extensibility
- Can handle growth in data/users
- Easy to add new features
- Configuration externalized where appropriate
- No hard-coded limits that will break

**Output**: Architectural concerns with recommendations

---

### Phase 4: Testing Review

**Purpose**: Ensure adequate test coverage, quality, and TDD compliance.

#### TDD Compliance (FIRST CHECK)

**Verify tests were written BEFORE implementation:**

**Questions to ask:**

1. **Does each function/method have corresponding tests?**
   ```
   For each public function in implementation files:
   - [ ] Test file exists
   - [ ] Test covers function behavior
   - [ ] Test name clearly describes what's tested
   ```

   **If NO**: Flag as Important issue
   ```
   IMPORTANT: Missing tests for [function_name] in [file].
   All functions require tests. Add tests before merging.
   ```

2. **Can author attest to RED-GREEN-REFACTOR cycle?**
   ```
   Ask: "For each function, did you:
   1. Watch test fail first? (RED)
   2. Implement minimal code to pass? (GREEN)
   3. Refactor for quality? (REFACTOR)"
   ```

   **If author cannot attest**: Flag as Important issue
   ```
   IMPORTANT: Unable to verify TDD followed for [function_name].
   Tests may have been written after implementation.
   Recommend: Verify tests actually fail when implementation removed.
   ```

3. **Do tests fail when implementation removed?**

   **If suspicious that tests written after** (e.g., all tests pass immediately, no git history showing test-first):

   ```bash
   # Verify tests actually test implementation
   # Comment out implementation
   git stash  # Temporarily remove implementation
   npm test   # Tests should FAIL
   git stash pop  # Restore implementation
   ```

   **If tests still pass with no implementation**: Flag as Critical issue
   ```
   CRITICAL: Tests for [function_name] pass without implementation.
   Tests are not actually testing the code.
   See testing-anti-patterns skill (Anti-Pattern 1: Testing Mock Behavior).
   Rewrite tests to verify real behavior.
   ```

4. **Does git history suggest test-first?**

   **Optional check** (not definitive, but helpful indicator):

   ```bash
   git log --oneline --all -- tests/[file].test.ts src/[file].ts
   ```

   **Look for:**
   - Test commits before implementation commits
   - Separate commits for RED → GREEN → REFACTOR
   - Commit messages mentioning TDD phases

   **If commits show implementation before tests**: Flag as Important issue
   ```
   IMPORTANT: Git history suggests tests written after implementation.
   Commits show [file].ts before [file].test.ts.
   Verify TDD was followed. If not, recommend rewriting with TDD.
   ```

5. **Are there any files with implementation but no tests?**

   ```bash
   # Find source files
   find src/ -name "*.ts" -o -name "*.js"

   # For each source file, check if test exists
   # tests/[name].test.ts or tests/[name].spec.ts
   ```

   **If source file has no corresponding test file**: Flag as Important issue
   ```
   IMPORTANT: No tests found for src/[file].ts
   All implementation files require tests.
   Add comprehensive tests before merging.
   ```

**TDD Compliance Summary:**

After checking above:

```markdown
### TDD Compliance: [PASS / NEEDS IMPROVEMENT / FAIL]

- Functions with tests: X / Y (Z% coverage)
- Author attestation to RED-GREEN-REFACTOR: [Yes / No / Partial]
- Tests verified to fail without implementation: [Yes / No / Not checked]
- Files without tests: [List or "None"]

[If NEEDS IMPROVEMENT or FAIL]:
Recommendations:
- Add tests for [list functions]
- Verify tests for [list functions] actually fail when implementation removed
- Consider rewriting [list functions] with TDD for higher confidence
```

#### Test Coverage
- All new code paths tested
- Edge cases covered
- Error paths tested
- Integration points verified

#### Test Quality
- Tests actually verify behavior (not just mock everything)
- Tests are isolated and independent
- Tests have clear arrange/act/assert structure
- Test names describe what they verify

#### Test Patterns and RED-GREEN-REFACTOR
- Appropriate use of mocks/stubs/fakes
- No test-only methods in production code
- Tests fail when they should (RED-GREEN-REFACTOR)

**See Also**: `testing-anti-patterns` skill for what NOT to do, `test-driven-development` skill for TDD process

**Output**: Testing gaps, quality issues, and TDD compliance status

---

### Phase 5: Security & Performance

**Purpose**: Identify vulnerabilities and performance issues.

**Security Review**:
- No SQL injection vulnerabilities
- No XSS vulnerabilities
- No command injection
- Proper authentication/authorization
- Sensitive data properly handled
- No secrets in code

**Performance Review**:
- No obvious N+1 queries
- Appropriate use of caching
- No unbounded loops or recursion
- Database queries optimized
- Async operations used appropriately

**Output**: Security and performance concerns

---

### Phase 6: Documentation Review

**Purpose**: Verify code is properly documented.

**Check**:
- Public APIs documented
- Complex algorithms explained
- Non-obvious decisions justified in comments
- README updated if needed
- Breaking changes documented

**Output**: Documentation gaps

---

## Issue Categorization

Categorize all findings by **Priority**:

### Critical (Must Fix Before Merge)
- Security vulnerabilities
- Data corruption risks
- Breaking existing functionality
- Missing core requirements
- Test failures

### Important (Should Fix Before Merge)
- Missing error handling
- Poor architecture that will cause problems
- Significant performance issues
- Missing important tests
- Plan deviations without justification

### Minor (Nice to Have)
- Style inconsistencies
- Minor optimizations
- Better naming suggestions
- Additional test coverage
- Documentation improvements

---

## Review Output Format

Structure your review as follows:

```markdown
# Code Review: [What Was Implemented]

## Summary

**Overall Assessment**: [Ready to merge / Needs fixes / Major revision needed]

**Strengths**:
- [What was done well]
- [Good decisions made]
- [Quality aspects worth noting]

**Issues Found**: [X] Critical, [Y] Important, [Z] Minor

---

## Critical Issues

### 1. [Issue Title]
**Location**: `file/path.ts:123-145`
**Issue**: [Specific problem with evidence]
**Impact**: [Why this is critical]
**Fix**: [Specific steps to resolve]

---

## Important Issues

### 1. [Issue Title]
**Location**: `file/path.ts:67`
**Issue**: [Specific problem]
**Why Important**: [Consequences if not fixed]
**Recommendation**: [How to fix]

---

## Minor Issues

### 1. [Issue Title]
**Location**: `file/path.ts:89`
**Suggestion**: [Improvement idea]
**Benefit**: [Why this would help]

---

## Plan Alignment

**Planned**: [What the plan said to do]
**Implemented**: [What was actually done]
**Deviations**:
- [Deviation 1]: [Justified/Problematic and why]
- [Deviation 2]: [Assessment]

---

## Testing Review

### TDD Compliance: [PASS / NEEDS IMPROVEMENT / FAIL]

**Findings:**
- Functions with tests: X / Y (Z% coverage)
- Author attestation to TDD: [Yes / No / Partial]
- Tests verified to fail without implementation: [Yes / No / Not checked]
- Files without tests: [List or "None"]

**Issues identified:**
[List TDD compliance issues here]

### Test Coverage

**Coverage**: [Percentage or qualitative assessment]
**Gaps**:
- [Untested scenario 1]
- [Untested scenario 2]

**Test Quality**: [Assessment]

---

## Recommendations

**Immediate Actions** (before merge):
1. [Action 1]
2. [Action 2]

**Future Improvements** (can defer):
1. [Improvement 1]
2. [Improvement 2]

---

## Approval Status

- [ ] Critical issues resolved
- [ ] Important issues addressed or acknowledged
- [ ] Tests passing
- [ ] Documentation updated

**Status**: [Approved / Approved with minor items / Needs revision]
```

---

## Integration with Workflows

### Requesting Review

See `requesting-code-review` skill for how to invoke this review process.

**When to review**:
- After completing each task (subagent-driven development)
- After major features
- Before merging to main
- When stuck (fresh perspective)

### Receiving Review

See `receiving-code-review` skill for how to handle review feedback.

**Key principles**:
- Verify before implementing
- Ask for clarification if unclear
- Push back with technical reasoning if reviewer is wrong
- No performative agreement

### Integration with Other Skills

**Integration with:**
- `test-driven-development`: Reviewers verify TDD was followed
- `testing-anti-patterns`: Reference when tests smell wrong
- `verification-before-completion`: Ensure verification happened before review

---

## Communication Protocol

### When Providing Feedback

**Do**:
- Be specific with file:line references
- Provide evidence for claims
- Explain why something is a problem
- Suggest concrete fixes
- Acknowledge what's working well

**Don't**:
- Make vague criticisms
- Assume without checking code
- Nitpick style without reason
- Focus only on negatives
- Use subjective language without backing

### If Original Plan Has Issues

Sometimes the implementation reveals flaws in the plan itself.

**If you find**:
- Requirements were incomplete
- Planned approach won't work
- Better approach exists

**Then**:
- Note this explicitly in review
- Recommend plan updates
- Acknowledge implementation did best with what it had
- Suggest next steps for course correction

---

## Review Checklist

Before submitting review, verify:

- [ ] **Phase 4 - TDD Compliance**:
  - [ ] All functions have corresponding tests
  - [ ] Author attested to RED-GREEN-REFACTOR cycle
  - [ ] No files without tests (or documented exceptions)
  - [ ] Tests appear to verify real behavior (not mocks)
- [ ] **Phase 4 - Test Coverage**: Adequate coverage, edge cases tested
- [ ] **Phase 4 - Test Quality**: Tests verify behavior, not mocks
- [ ] Reviewed all changed files
- [ ] Checked against plan/requirements
- [ ] Looked for security issues
- [ ] Considered performance
- [ ] Checked error handling
- [ ] Verified documentation
- [ ] Categorized issues by priority
- [ ] Provided specific file:line references
- [ ] Suggested actionable fixes
- [ ] Acknowledged strengths

---

## Common Review Patterns

### Pattern: Missing Error Handling

**Finding**:
```typescript
// file.ts:45
const data = await fetchData(id);
return processData(data);
```

**Issue**: No error handling if fetchData fails

**Fix**:
```typescript
try {
  const data = await fetchData(id);
  return processData(data);
} catch (error) {
  logger.error('Failed to fetch data', { id, error });
  throw new DataFetchError(`Failed to fetch data for ${id}`, { cause: error });
}
```

---

### Pattern: Plan Deviation

**Planned**: Use Redis for caching
**Implemented**: In-memory cache with LRU eviction

**Assessment**:
- **Problematic** if multi-instance deployment (cache not shared)
- **Acceptable** if single instance and plan didn't specify why Redis

**Recommendation**: Clarify deployment model. If multi-instance, implement Redis as planned.

---

### Pattern: Test Quality Issue

**Finding**:
```typescript
// Tests mock everything, never test actual integration
it('should process order', () => {
  const mockDb = { save: jest.fn() };
  const mockPayment = { charge: jest.fn() };
  // ... everything mocked
});
```

**Issue**: Tests verify mocks work, not actual functionality

**Fix**: Add integration tests that test real behavior with test database/payment sandbox

**See**: `testing-anti-patterns` skill

---

### Example: TDD Compliance Review

```markdown
## Phase 4: Testing Review

### TDD Compliance: NEEDS IMPROVEMENT

**Findings:**
- Functions with tests: 8 / 10 (80% coverage)
- Author attestation to TDD: Partial (6 functions attested, 2 uncertain)
- Tests verified to fail without implementation: No (trusted author)
- Files without tests: src/utils/validation.ts

**Issues identified:**

**IMPORTANT #3**: Missing tests for validation.ts
File: src/utils/validation.ts
Issue: No tests found for validation utility functions
Impact: validateEmail, validatePassword not tested
Recommendation: Add comprehensive tests before merging

**IMPORTANT #4**: Unable to verify TDD for retryOperation
File: src/api/retry.ts
Issue: Author uncertain if test was written before implementation
Git history shows retry.ts committed before retry.test.ts
Recommendation:
1. Verify test fails when retryOperation implementation removed
2. If test doesn't fail, rewrite test to verify real behavior
3. Consider rewriting function with TDD for higher confidence

**Recommendations:**
1. Add tests for validation.ts (5 functions need tests)
2. Verify retry.test.ts actually tests retry.ts behavior
3. For future work, commit tests before implementation (clearer TDD evidence)
```

---

## Advanced Scenarios

### Reviewing Refactoring

When reviewing refactors:
1. Verify behavior unchanged (tests still pass)
2. Check if complexity actually reduced
3. Ensure no new bugs introduced
4. Validate performance not degraded
5. Confirm readability improved

### Reviewing Bug Fixes

When reviewing fixes:
1. Verify fix addresses root cause, not symptom
2. Check if similar bugs exist elsewhere
3. Ensure fix doesn't break other cases
4. Validate tests added to prevent regression
5. Consider if architecture change needed to prevent class of bugs

### Reviewing New Features

When reviewing features:
1. Check all acceptance criteria met
2. Verify edge cases handled
3. Ensure consistent with existing features
4. Check if feature is actually needed (YAGNI)
5. Validate UX is reasonable

---

## Quality Bar

### Excellent Implementation
- Meets all requirements
- High test coverage with quality tests
- Clean, maintainable code
- Proper error handling
- Well documented
- No security issues
- Good performance

### Acceptable Implementation
- Meets core requirements
- Adequate test coverage
- Readable code
- Basic error handling
- Security basics covered
- No major performance issues
- Minor improvements possible

### Needs Revision
- Missing requirements
- Inadequate tests
- Hard to understand
- Poor error handling
- Security concerns
- Performance problems
- Architectural issues

---

## Success Criteria

A successful code review:

✅ Identifies all critical issues that must be fixed
✅ Provides specific, actionable feedback
✅ Acknowledges what was done well
✅ Categorizes issues by priority
✅ Includes file:line references
✅ Suggests concrete improvements
✅ Helps improve both current code and future practices

---

*See also*: `requesting-code-review`, `receiving-code-review`, `testing-anti-patterns`, `verification-before-completion`
